# Abstract

- ViT在视觉任务上取得了巨大的成功，但是由于其沉重的计算成本限制了其处理大型特征图的能力
- 先前的工作要么依赖于限制在局部小区域的细粒度自关注，要么依赖于全局自关注来缩短序列，从而产生粗粒度
  - 粗粒度：综合理解文章的整体信息，如文本的大致结构和主旨
  - 细粒度：细粒度处理可能涉及注意力机制对特定单词或短语间复杂关系的捕捉，如上下文中的具体意义、语义细节等
- 我们的模型可以自适应细粒度的有效全局自注意
- 我们将更多的token分配给显著性区域以实现细粒度注意力，将更少的token分配给次要区域换取效率和全局感受场

# Introduction

- 第一段
  - ViT的开创性工作引入了自我注意模块，对图像之间的长程依赖性进行建模，克服了卷积操作局部感受野的固有限制
- 第二段
  - 为了降低计算成本，ViT采用大步长的patch embedding来减少序列长度。
  - 然而，这样的操作不可避免地导致自关注仅适用于具有粗粒度的小尺寸特征图，因为大步长减少了重叠，导致提取的特征更加粗糙
  - 为了计算高分辨率特征的自注意，提出一些方法，将自注意区域限制为局部窗口，而不是整个特征图（即细粒度局部自注意）
- 第三段
  - 直观地说，我们倾向于将更多的令牌分配给显著区域，这样每个token都可以以精细的粒度与显著区域交互，同时在次要区域上分配更少的令牌，这对于效率来说是不可变的
  - SG-Former利用全局感受野有效地估计自注意，并自适应地处理显著区域的细粒度信息
- 第四段
  - 这种方法通过混合尺度自注意力估计图像区域的重要性，并利用这些信息来指导令牌的重新分配。每个图像都有专门为它定制的令牌重新分配方案，减少了人为先验的影响。
  - 此外，该方法通过在一个层内处理不同粒度的信息，并为变换器提供混合尺度信息，从而保持了与Swin模型相同的计算成本。



# Method

