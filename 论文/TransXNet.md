# Abstract

- 问题
  - 传统卷积的静态性质无法动态适应输入变化，导致卷积和自注意力之间存在变化
  - 当堆叠由卷积和自注意力组成的混合标记器形成深层网络时，卷积的静态性质阻碍了先前的由自注意力生成的特征融合到卷积核中
- 提出
  - 提出了D-Mixer，可以聚合全局信息和局部细节
  - D-Mixer的工作原理是在均匀分割的特征片段上分别应用高效的全局注意力模块和依赖于输入的深度卷积，赋予网络强大的感应偏置和扩大的缺陷感受野。

# Introduction

- 第一段
  - VIT在计算机视觉领域展现出了很有前途的进步
  - 但是它不能像卷积那样有归纳偏差，所以泛化能力差
  - 为了解决这个问题，引入了Swin-Transformer，引入了移位窗口自注意力，这样结合了归纳偏差并且降低了多头自注意力的计算成本
  - 由于窗口的注意力的局部性质，Swin-Transformer的感受野有限
- 第二段
  - 尽管有很多研究让Vit获得归纳偏差，将自注意力和卷积融合到一起，但是这样的性能提升是有限的
  - 这种局限性主要有两个原因：
    - 标准的卷积核是独立于输入的，无法适应不同的输入，导致卷积和自注意力在表示能力上的差异，削弱了hybrid token mixers的建模能力
    - 随着通过堆叠hybrid token mixers，网络变得更深，自注意力能够动态地整合前面块中由卷积生成的特征，而卷积的静态性则阻碍了它有效地整合和利用自我注意力之前生成的特征。
- 第三段
  - 网络也应该有一个大的接受场和归纳偏差来捕捉丰富的上下文信息
  - 作者通过实验比较，为了获得较大的感受野，应该将高效的全局自注意力机制整合到网络的所有阶段
  - 将动态卷积和全局自注意力机制结合可以进一步扩大感受野
- 第四段-介绍D-Mixer
  - **D-Mixer的设计**：D-Mixer将输入特征分为两部分，分别通过“重叠空间降维注意力”模块（Overlapping Spatial Reduction Attention module）和“输入依赖的深度卷积”（Input-dependent Depthwise Convolution）处理，然后将这两个输出连接在一起。这种设计既能让网络捕捉到全局上下文信息，又能有效地引入归纳偏差。
  - **大有效感受野（ERF）**：D-Mixer在实现大的有效感受野方面表现出色，同时具有显著的局部敏感性和非局部注意力。
  - **多尺度前馈网络（MS-FFN）**：引入了一种多尺度前馈网络，用于在token聚合过程中探索多尺度信息。
  - **TransXNet架构**：通过层叠D-Mixer和MS-FFN构建的基础块，形成了一个多功能的视觉识别主干网络，称为TransXNet。TransXNet在ImageNet1K图像分类任务中展示了卓越的性能，与最新的先进方法相比，表现更优。
  - **性能优势**：TransXNetT实现了81.6%的顶级精度，计算成本仅为1.8G FLOPs和12.8M参数，优于Swin-T模型，而计算成本不到其一半。TransXNet-S/B模型实现了83.8%/84.6%的顶级精度，超过了最近提出的InternImage模型，且计算成本更低。
  - **主要贡献**：提出了D-Mixer，有效地聚合了稀疏的全局信息和局部细节；设计了强大的TransXNet视觉主干网络；在图像分类、目标检测以及语义和实例分割任务上进行了广泛的实验，显示出优于之前方法的性能和较低的计算成本。

# Related Work

- 卷积神经网络
  - 现在的CNN已经放弃了3*3，逐渐采用大核的卷积核
- Vit
  - Transformer在自然语言处理中很优秀
  - MHSA可以对token依赖性建模
  - 但是在处理高分辨率的图片输入时处理成本很高，所以很多工作采取了具有金字塔设计的高效注意力机制
- CNN-Transformer
  - 纯transformer缺少归纳偏差，所以混合这两者是一个可行的方案
  - 一些模型在浅层使用CNN，深层使用Transformer
- Dynamic Weight
  - 动态权重是自注意力优越性的一个强大因素，能够根据输入动态的提取特征，还有长距离建模的能力
  - 动态卷积能够更好的提取局部性特征
  - 有研究表明，用动态卷积替换Swin Transformer中的移动窗口可以以更低的计算成本获得更好的结果。
  - 我们的工作是为了构建一个混合模型，使模型有更大的感受野和归纳偏差

# Method

TransXNet包含四个阶段。每个阶段由一个补丁嵌入层和若干顺序堆叠的块组成。第一个patch embendding采用7×7卷积层（步长为4），后接批量归一化（BN），而剩余阶段的patch embedding则使用3×3卷积层（步长为2）配合BN。每个块包含一个动态位置编码（DPE）层、一个双重动态令牌混合器（D-Mixer）和一个多尺度前馈网络（MS-FFN）。

- Dual Dynamic Token Mixer
  - 为了引入归纳偏差来增加模型的泛化能力，很多之前的方法将卷积和自注意力相结合
  - 然而他们的方法稀释了Transformer的输入依赖性。
  - 我们提出的D-Mixer，可以动态的利用全局和局部信息，在不影响输入依赖性的情况下拥有大的感受野和强大的归纳偏差
  - 沿着通道方向将C均匀分成两部分X1,X2,X1会输入到一个全局自注意力的ORSA模块，X2输入到一个动态卷积模块，再将这两个模块进行拼接，最后使用一个STE进行局部的token融合
  - D-Mixer示意图：
  - ![image-20231209090429677](https://pj-typora.oss-cn-shanghai.aliyuncs.com/image-20231209090429677.png)

- IDConv
  - 采用自适应平均池化来聚合空间上下文，压缩空间维度至K²，然后将其传递至两个连续的1x1卷积，产生注意力图A' ∈ R^(G×C)×K²，其中G表示注意力组的数量。然后，A'被重塑为R^G×C×K²形状，并使用Softmax函数在G维度上运算，从而生成注意力权重A ∈ R^G×C×K²。最终，A与一组可学习参数P ∈ R^G×C×K²进行逐元素乘法，输出结果在G维度上求和，产生输入依赖的深度卷积核W ∈ R^C×K²
  - ![image-20231209091108963](https://pj-typora.oss-cn-shanghai.aliyuncs.com/image-20231209091108963.png)
- OSRA
  - 提取全局信息
- STE
  - 在执行token混合之后，大多数之前的模型会使用1 * 1的卷积来实现跨信道通信，这可能产生额外的计算开销
  - ![image-20231209093306190](https://pj-typora.oss-cn-shanghai.aliyuncs.com/image-20231209093306190.png)

- MS-FFN

  - 在普通的FFN中，通过在隐藏层引入3×3深度卷积，可以实现局部标记聚合
  - 然而，由于隐藏层的通道数更多，通常是输入通道数的四倍，单一尺度的标记聚合无法充分利用这种丰富的通道表示。为了解决这个问题，提出了MS-FFN。
  - MS-FFN通过引入不同尺寸的深度卷积核来处理不同尺度的信息，从而更有效地利用隐藏层中丰富的通道表示
  - 这种多尺度方法允许网络同时捕捉到小尺度（局部）和大尺度（全局）的特征，增强了模型的表示能力

  