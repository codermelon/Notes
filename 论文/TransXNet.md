# Abstract

- 问题
  - 传统卷积的静态性质无法动态适应输入变化，导致卷积和自注意力之间存在变化
  - 当堆叠由卷积和自注意力组成的混合标记器形成深层网络时，卷积的静态性质阻碍了先前的由自注意力生成的特征融合到卷积核中
- 提出
  - 提出了D-Mixer，可以聚合全局信息和局部细节
  - D-Mixer的工作原理是在均匀分割的特征片段上分别应用高效的全局注意力模块和依赖于输入的深度卷积，赋予网络强大的感应偏置和扩大的缺陷感受野。

# Introduction

- 第一段
  - VIT在计算机视觉领域展现出了很有前途的进步
  - 但是它不能像卷积那样有归纳偏差，所以泛化能力差
  - 为了解决这个问题，引入了Swin-Transformer，引入了移位窗口自注意力，这样结合了归纳偏差并且降低了多头自注意力的计算成本
  - 由于窗口的注意力的局部性质，Swin-Transformer的感受野有限
- 第二段
  - 尽管有很多研究让Vit获得归纳偏差，将自注意力和卷积融合到一起，但是这样的性能提升是有限的
  - 这种局限性主要有两个原因：
    - 标准的卷积核是独立于输入的，无法适应不同的输入，导致卷积和自注意力在表示能力上的差异，削弱了hybrid token mixers的建模能力
    - 随着通过堆叠hybrid token mixers，网络变得更深，自注意力能够动态地整合前面块中由卷积生成的特征，而卷积的静态性则阻碍了它有效地整合和利用自我注意力之前生成的特征。
- 第三段
  - 网络也应该有一个大的接受场和归纳偏差来捕捉丰富的上下文信息
  - 作者通过实验比较，为了获得较大的感受野，应该将高效的全局自注意力机制整合到网络的所有阶段
  - 将动态卷积和全局自注意力机制结合可以进一步扩大感受野
- 第四段-介绍D-Mixer
  - **D-Mixer的设计**：D-Mixer将输入特征分为两部分，分别通过“重叠空间降维注意力”模块（Overlapping Spatial Reduction Attention module）和“输入依赖的深度卷积”（Input-dependent Depthwise Convolution）处理，然后将这两个输出连接在一起。这种设计既能让网络捕捉到全局上下文信息，又能有效地引入归纳偏差。
  - **大有效感受野（ERF）**：D-Mixer在实现大的有效感受野方面表现出色，同时具有显著的局部敏感性和非局部注意力。
  - **多尺度前馈网络（MS-FFN）**：引入了一种多尺度前馈网络，用于在token聚合过程中探索多尺度信息。
  - **TransXNet架构**：通过层叠D-Mixer和MS-FFN构建的基础块，形成了一个多功能的视觉识别主干网络，称为TransXNet。TransXNet在ImageNet1K图像分类任务中展示了卓越的性能，与最新的先进方法相比，表现更优。
  - **性能优势**：TransXNetT实现了81.6%的顶级精度，计算成本仅为1.8G FLOPs和12.8M参数，优于Swin-T模型，而计算成本不到其一半。TransXNet-S/B模型实现了83.8%/84.6%的顶级精度，超过了最近提出的InternImage模型，且计算成本更低。
  - **主要贡献**：提出了D-Mixer，有效地聚合了稀疏的全局信息和局部细节；设计了强大的TransXNet视觉主干网络；在图像分类、目标检测以及语义和实例分割任务上进行了广泛的实验，显示出优于之前方法的性能和较低的计算成本。
- 