# Method

![image-20240228131506060](https://pj-typora.oss-cn-shanghai.aliyuncs.com/image-20240228131506060.png)

- Large Kernel Attention
  - 该机制利用大型卷积核模拟自注意力机制典型的广泛接收字段，但显著减少了计算开销。
  - 这种方法结合了深度卷积和膨胀卷积，通过最终的1x1卷积高效构建大核心。该方法在减少参数和计算需求的同时，确保了模型对不同数据模式的适应性，这对于处理医学图像中常见的复杂形状和大小至关重要。
- Deformable Large Kernel Attention
  - 进一步扩展了大核心注意力的概念，通过结合变形卷积（Deformable Convolutions），使得采样网格能够自由变形，从而适应不同的数据模式。变形卷积通过额外的卷积层学习特征图上的位移场（offset field），实现基于特征的自适应卷积核形状，这种灵活的核心形状能够更好地表征如病变或器官变形等情况，从而提高对象边界的定义。
  - 在变形大核心注意力机制中，通过以下步骤实现了一种自适应的大卷积核：
    1. **利用变形卷积**：变形卷积使得采样点能够根据学习到的偏移量自由移动，适应不同的形状和尺寸，这对于医学图像中常见的不规则对象尤为重要。
    2. **位移场的学习**：通过额外的卷积层学习位移场，使得卷积核可以根据输入特征的需要进行自适应调整，提高了模型对特定数据模式的适应性。
    3. **双线性插值**：对于位移后不在图像网格上的采样点，使用双线性插值计算其像素值，保证变形后的采样点能够有效地融入原始图像中。
  - 