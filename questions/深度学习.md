# 1. 梯度下降

- 让计算机根据x推出y，需要一个权重w

![image-20230917095507060](https://pj-typora.oss-cn-shanghai.aliyuncs.com/image-20230917095507060.png)

- 开始时，计算机会随机给到一个w，通过这个w计算出y^

![](https://pj-typora.oss-cn-shanghai.aliyuncs.com/image-20230917095918189.png)

- 想要求出最佳的w，就要更新参数w，寻找最好的w

![image-20230917100119588](https://pj-typora.oss-cn-shanghai.aliyuncs.com/image-20230917100119588.png)

![image-20230917100140698](https://pj-typora.oss-cn-shanghai.aliyuncs.com/image-20230917100140698.png)

## 1.1 随机梯度下降

随机梯度下降与梯度下降的区别：

随机梯度下降只需要计算一个样本，梯度下降是计算全部的样本的平均值

![image-20230917100838534](https://pj-typora.oss-cn-shanghai.aliyuncs.com/image-20230917100838534.png)

# 2 读取数据集

## 1. 读取数据集

epoch：跑完所有的训练集的样本的轮数

batch_size：一次前向传播读进去的样本数量

```python
import numpy as np
import torch
from torch.utils.data import Dataset,DataLoader

class DiabetesDataset(Dataset):
    def __init__(self,filepath):
        xy = np.loadtxt(filepath,delimiter=',',dtype=np.float32)
        self.len = xy.shape[0] # 获得样本的数量
        self.x_data = torch.from_numpy(xy[:,:-1])
        self.y_data = torch.from_numpy(xy[:,[-1]])

    def __getitem__(self, index):
        return self.x_data[index],self.y_data[index]

    def __len__(self):
        return self.len

dataset = DiabetesDataset('diabetes.csv.gz')
train_loader = DataLoader(dataset=dataset,
                          batch_size=32,
                          shuffle=True,
                          num_workers=2)

class Model(torch.nn.Module):
    def __init__(self):
        super(Model,self).__init__()
        self.linear1 = torch.nn.Linear(8,6)
        self.linear2 = torch.nn.Linear(6,4)
        self.linear3 = torch.nn.Linear(4,1)
        self.sigmoid = torch.nn.Sigmoid()

    def forwrd(self,x):
        x= self.sigmoid(self.linear1(x))
        x= self.sigmoid(self.linear2(x))
        x= self.sigmoid(self.linear3(x))
        return x

model = Model()

criterion = torch.nn.BCELoss(reduction='mean')
optimizer = torch.optim.SGD(model.parameters(), lr=0.1)

if __name__ =='_main_':
    for epoch in range(100):
        # 0表示从索引0开始
        for i,data in enumerate(train_loader,0):
            inputs,lables = data
            y_pred = model(inputs)
            loss = criterion(y_pred,lables)
            print(epoch,i,loss.item())
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

```

## 2. Minst数据集

```python
import torch
from torch.utils.data import DataLoader
from torchvision import transforms
from torchvision import datasets

train_dataset = datasets.MNIST(root='....',
                               train=True,
                               transform=transforms.ToTensor(),
                               download=True)

test_dataset = datasets.MNIST(root='...',
                              train=False,
                              transform=transforms.ToTensor,
                              download=True)

train_loader = DataLoader(dataset=train_dataset,
                          batch_size=32,
                          shuffle=True)
test_dataset = DataLoader(dataset=test_dataset,
                          batch_size=32,
                          shuffle=False)
```

# 3. 多分类问题

```python
import torch
from torchvision import transforms
from torchvision import datasets
from torch.utils.data import DataLoader
import torch.nn.functional as F
import torch.optim as optim

batch_size = 64
transform = transforms.Compose([
    transforms.ToTensor(),
    # 变成张量，满足c*w*h
    transforms.Normalize((0.1307),(0.3081))
])

train_dataset = datasets.MNIST(root='....',
                               train=True,
                               transform=transform,
                               download=True)

test_dataset = datasets.MNIST(root='...',
                              train=False,
                              transform=transform,
                              download=True)

train_loader = DataLoader(dataset=train_dataset,
                          batch_size=batch_size,
                          shuffle=True)
test_loader = DataLoader(dataset=test_dataset,
                          batch_size=batch_size,
                          shuffle=False)

class Net(torch.nn.Module):
    def __init__(self):
        super(Net,self).__init__()
        self.l1=torch.nn.Linear(784,512)
        self.l2=torch.nn.Linear(512,256)
        self.l3=torch.nn.Linear(256,128)
        self.l4=torch.nn.Linear(128,64)
        self.l5=torch.nn.Linear(64,10)

    def forward(self,x): # x表示包含输入样本的张量
        x = x.view(-1,784) # 转为2D
        x= F.relu(self.l1(x))
        x= F.relu(self.l2(x))
        x= F.relu(self.l3(x))
        x= F.relu(self.l4(x))
        return self.l5(x)

model = Net()

criterion = torch.nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)

def train(epoch):
    running_loss = 0.0
    for batch_idx,data in enumerate(train_loader,0):
        inputs,target = data
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs,target)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        if batch_idx % 300 == 299:
            print('[%d, %5d] loss: %.3f' % (epoch + 1, batch_idx + 1, running_loss / 300))
            running_loss = 0.0

def test():
    correct = 0
    total =0
    with torch.no_grad(): # 不计算梯度信息
        for data in test_loader:
            images,labels = data
            outputs = model(images)
            _, predicted = torch.max(outputs,dim=1)
            total += labels.size(0) # 在pytorch中通常将批次大小作为张量的第一个维度，可以获取当前样本数量
            correct += (predicted == labels).sum().item()
    print('Accuracy on test set: %d %%' % (100 * correct / total))
```

# 4. CNN

## 1. 基本流程：

```python
import torch
in_channels,out_channels = 5, 10
width,height = 100, 100
kernel_size = 3
batch_size = 1

imput = torch.randn(
    batch_size,
    in_channels,
    width,
    height
)

conv_layer = torch.nn.Conv2d(in_channels,
                             out_channels,
                             kernel_size=kernel_size)

output = conv_layer(input)

print(input.shape)
print(output.shape)
print(conv_layer.weight.shape)
```

## 2. 网络结构

```python
class Net(torch.nn.Module):
    def __init__(self):
        super(Net,self).__init__()
        self.conv1 = torch.nn.Conv2d(1,10,kernel_size=5)
        self.conv2 = torch.nn.Conv2d(10,20,kernel_size=5)
        self.pooling = torch.nn.MaxPool2d(2)
        self.fc = torch.nn.Linear(320,10)

    def forward(self,x):
        batch_size = x.size(0)
        x = F.relu(self.pooling(self.conv1(x)))
        x = F.relu(self.pooling(self.conv2(x))) # 此处x的形状是(batch_size,c,h,w)
        x = x.view(batch_size,-1) # 此处x的形状是(batch_size,c*h*w)
        x = self.fc(x)
        return x

model = Net()
# 如果要使用gpu
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
model.to(device)
```

**理解batch_size = x.size(0)：**

这行代码 `batch_size = x.size(0)` 的目的是获取输入张量 `x` 的批量大小（也就是一次性处理的样本数量）。为了更好地理解这行代码，我们可以结合代码和运行结果进行解释。

在深度学习中，通常会将数据划分成批次进行训练，而不是逐个样本处理。每个批次包含一定数量的样本，这个数量就是批量大小（batch size）。在代码中，`x` 表示输入数据，通常是一个包含多个图像样本的张量，其中每个样本都有相同的特征维度和形状。

让我们假设 `x` 是一个包含4张图像的批次，每张图像的形状为 `(1, 28, 28)`，也就是单通道的灰度图像，高度和宽度都是28像素。那么在这种情况下，`x` 的形状可能是 `(4, 1, 28, 28)`，其中第一个维度（0号维度）表示批量大小为4，即有4张图像。

通过执行 `batch_size = x.size(0)` 这行代码后，`batch_size` 的值将被设置为4，因为它获取了 `x` 张量的第0号维度的大小，这正是批量大小的值。换句话说，`batch_size` 变量现在包含了这个批次中的样本数量。

这个值在后续的代码中可能会用于各种目的，例如确定模型的输出形状或计算损失函数时会考虑到批次的大小。在深度学习中，批次大小是一个重要的超参数，通常需要根据问题的性质和硬件资源进行调整。

**x = x.view(batch_size,-1)：**

`x = x.view(batch_size, -1)` 这行代码的目的是对输入张量 `x` 进行维度重塑，将其转换为新的形状，以便进行后续的计算。让我们根据代码和运行结果解释这行代码的作用：

假设我们有一个包含4张图像的批次，每张图像的形状为 `(1, 28, 28)`，如前面的例子所述，而且已经获得了 `batch_size` 的值为4，表示这个批次包含4个样本。

- 初始状态下，`x` 的形状是 `(4, 1, 28, 28)`，表示有4个样本，每个样本是单通道的28x28像素图像。

- `x.view(batch_size, -1)` 这行代码的目的是将 `x` 重塑成一个新的形状，其中 `batch_size` 保持不变（4），而 `-1` 的作用是自动计算缺失的维度大小，以确保总的元素数量不变。在这里，`-1` 的位置将根据张量的大小和 `batch_size` 的值自动计算。

如果我们将 `x` 重塑为 `x.view(batch_size, -1)`，那么结果将是一个形状为 `(4, 1 * 28 * 28)` 的张量，也就是 `(4, 784)`。这表示我们将每个图像样本展平成一个包含784个元素的一维向量。

这种展平操作通常在卷积神经网络（CNN）的卷积层之后，以便将特征图转换为全连接层的输入。在这个例子中，它将4张28x28的图像转换为4个长度为784的向量，以便将它们传递给全连接层进行分类或其他后续任务的处理。展平操作有助于将图像数据转换为适合全连接层的输入形状。



# 5. 在模型中载入部分权重

- 方法1:添加一个全连接层，因为已经下载好了权重对应的是1000分类的权重 替换掉原来的全连接层
- 此处载入的权重是除了最后一个全连接层外其他的权重

1. 先创建resnet34网络，此处分类任务仍然是1000分类
2. 导入官方的权重
3. 获得原来的全连接层的输入节点个数
4. 通过nn.Linear创建一个新的全连接层替换原来的全连接层

```python
# load pretrain weights
    # download url: https://download.pytorch.org/models/resnet34-333f7ec4.pth
    model_weight_path = "./resnet34-pre.pth"
    assert os.path.exists(model_weight_path), "file {} does not exist.".format(model_weight_path)
    
# option1
net = resnet34()
net.load_state_dict(torch.load(model_weight_path, map_location=device))
# change fc layer structure
in_channel = net.fc.in_features
net.fc = nn.Linear(in_channel, 5)
```

- 方法2:一开始就修改分类的个数，不能直接导入原来的权重，因为分类的个数不一致

1. 先读取我们的权重pre_weights，以一个有序字典的形式存储，pre_weights中存储的是每个层次的权重
2. 还可以通过net_weights = net.state_dict()获得各部分的权重

```python
# option2
net = resnet34(num_classes=5)
pre_weights = torch.load(model_weight_path, map_location=device)
del_key = []
for key, _ in pre_weights.items():
    if "fc" in key:
        del_key.append(key)

for key in del_key:
    del pre_weights[key]

missing_keys, unexpected_keys = net.load_state_dict(pre_weights, strict=False)
print("[missing_keys]:", *missing_keys, sep="\n")
print("[unexpected_keys]:", *unexpected_keys, sep="\n")
```

- 通过遍历循环pre_weights中每一个key值，如果key中包含了fc字段，可以把fc字段删除



# 6. 自定义数据集

# 7. os文件操作

- os.listdir(root):遍历root下所有的文件和文件夹
- os.path.isdir():判断是否为目录

